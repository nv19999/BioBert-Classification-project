{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ9zZYZxoxqo",
        "outputId": "b3cd57a3-ee0a-4cd3-ba90-4f11e19c4538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 11550\n",
            "Synthetic dataset size: 15000\n",
            "Combined dataset size: 26550\n",
            "\n",
            "First few rows of combined dataset:\n",
            "   condition_label                                   medical_abstract\n",
            "0                1  Extended neck dissection. From the time Crile ...\n",
            "1                5  Thoracoplasty: current application to the infe...\n",
            "2                3  Recurrent tension headache in adolescents trea...\n",
            "3                1  Intraoperative pancreatic fine needle aspirati...\n",
            "4                1  Presence of identical mitochondrial proteins i...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "folder_path_real = \"../dataset/\"\n",
        "folder_path_synthetic = \"../dataset/\"\n",
        "# Read the datasets\n",
        "real_df = pd.read_csv(f\"{folder_path_real}/medical_tc_train.csv\")\n",
        "synthetic_df = pd.read_csv(f\"{folder_path_synthetic}/Simpler_Augmented_Synthetic_Dataset.csv\")\n",
        "\n",
        "# Combine and shuffle the datasets\n",
        "combined_df = pd.concat([real_df, synthetic_df], ignore_index=True)\n",
        "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "output_path = f\"{folder_path_real}/combined_medical_dataset.csv\"\n",
        "combined_df.to_csv(output_path, index=False)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Original dataset size: {len(real_df)}\")\n",
        "print(f\"Synthetic dataset size: {len(synthetic_df)}\")\n",
        "print(f\"Combined dataset size: {len(combined_df)}\")\n",
        "\n",
        "\n",
        "print(\"\\nFirst few rows of combined dataset:\")\n",
        "print(combined_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMuWf3eO0wkQ",
        "outputId": "77f1e4e2-e03a-40dd-a8de-999a1bfa8ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows (Training + Test): 29438\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "folder_path = \"../dataset/\"\n",
        "\n",
        "training_total = len(pd.read_csv(f\"{folder_path}/medical_tc_train.csv\")) + \\\n",
        "                 len(pd.read_csv(f\"{folder_path}/Simpler_Augmented_Synthetic_Dataset.csv\"))\n",
        "test_total = len(pd.read_csv(f\"{folder_path}/medical_tc_test.csv\"))\n",
        "\n",
        "print(f\"Total number of rows (Training + Test): {training_total + test_total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyfEuPKe2hvB",
        "outputId": "23a44666-36f7-473b-e71c-7fb742e887b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "Total samples: 26550\n",
            "Number of unique conditions: 5\n",
            "\n",
            "Label distribution:\n",
            "condition_label\n",
            "1    5694\n",
            "2    2810\n",
            "3    3621\n",
            "4    5615\n",
            "5    8810\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Preprocessing for BioBERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning medical abstracts...\n",
            "Encoding texts with BioBERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26550/26550 [00:30<00:00, 877.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing complete!\n",
            "Input shape: torch.Size([26550, 512])\n",
            "Number of labels: 26550\n",
            "\n",
            "Unique labels in processed data: [0, 1, 2, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "def clean_clinical_text(text):\n",
        "    \"\"\"Clean clinical text by removing special characters and extra whitespace.\"\"\"\n",
        "    text = str(text)\n",
        "    # Remove special characters but keep important punctuation\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\?\\!]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text.lower().strip()\n",
        "\n",
        "def preprocess_for_biobert(df, max_length=512):\n",
        "    \"\"\"Preprocess clinical text data for BioBERT\"\"\"\n",
        "    # Load BioBERT tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-v1.1', device_map=\"auto\")\n",
        "\n",
        "    # Clean the texts\n",
        "    print(\"Cleaning medical abstracts...\")\n",
        "    df['medical_abstract'] = df['medical_abstract'].apply(clean_clinical_text)\n",
        "\n",
        "    # Prepare lists for encoded data\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # Encode each text\n",
        "    print(\"Encoding texts with BioBERT tokenizer...\")\n",
        "    for text in tqdm(df['medical_abstract']):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(df['condition_label'].values - 1)\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_masks': attention_masks,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(f\"{folder_path}/combined_medical_dataset.csv\")\n",
        "\n",
        "\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n",
        "print(f\"Number of unique conditions: {df['condition_label'].nunique()}\")\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['condition_label'].value_counts().sort_index())\n",
        "\n",
        "\n",
        "print(\"\\nPreprocessing for BioBERT...\")\n",
        "preprocessed_data = preprocess_for_biobert(df)\n",
        "\n",
        "print(\"\\nPreprocessing complete!\")\n",
        "print(f\"Input shape: {preprocessed_data['input_ids'].shape}\")\n",
        "print(f\"Number of labels: {len(preprocessed_data['labels'])}\")\n",
        "\n",
        "print(\"\\nUnique labels in processed data:\", torch.unique(preprocessed_data['labels']).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHuuiQh27JSp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import SGD, AdamW\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "def create_data_loaders(input_ids, attention_masks, labels, batch_size=32, val_split=0.1):\n",
        "    \"\"\"Create train and validation dataloaders\"\"\"\n",
        "    # Combine into dataset\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    # Calculate lengths for split\n",
        "    val_len = int(len(dataset) * val_split)\n",
        "    train_len = len(dataset) - val_len\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "def setup_model(num_labels=5):\n",
        "    \"\"\"Initialize BioBERT model for classification\"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        'dmis-lab/biobert-v1.1',\n",
        "        num_labels=num_labels,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, epochs=5):\n",
        "    # Set up optimizer\n",
        "    optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.0001)\n",
        "\n",
        "    # Total steps for scheduler\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Create scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=1000,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    best_accuracy = 0\n",
        "    training_stats = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\n======== Epoch {epoch + 1} / {epochs} ========')\n",
        "        print('Training...')\n",
        "\n",
        "        # Reset tracking variables\n",
        "        total_train_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        # Training\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Progress update\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}')\n",
        "\n",
        "            # Unpack batch and copy to GPU\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(b_input_ids,\n",
        "                          attention_mask=b_input_mask,\n",
        "                          labels=b_labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate average loss for this epoch\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n",
        "\n",
        "        # Validation\n",
        "        print(\"\\nRunning Validation...\")\n",
        "        model.eval()\n",
        "\n",
        "        # Tracking variables for validation\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in val_dataloader:\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(b_input_ids,\n",
        "                              attention_mask=b_input_mask,\n",
        "                              labels=b_labels)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            accuracy = (predictions == b_labels).float().mean()\n",
        "\n",
        "            total_eval_accuracy += accuracy.item()\n",
        "            total_eval_loss += outputs.loss.item()\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        avg_val_accuracy = total_eval_accuracy / nb_eval_steps\n",
        "        avg_val_loss = total_eval_loss / nb_eval_steps\n",
        "\n",
        "        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
        "        print(f\"  Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
        "\n",
        "        if avg_val_accuracy > best_accuracy:\n",
        "            best_accuracy = avg_val_accuracy\n",
        "            torch.save(model.state_dict(), '/content/drive/My Drive/best_biobert_medical_classifier.pt')\n",
        "            print(f\"  New best model saved! Accuracy: {best_accuracy:.2f}\")\n",
        "\n",
        "        training_stats.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "        })\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    return model, training_stats, best_accuracy\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader, val_dataloader = create_data_loaders(\n",
        "    preprocessed_data['input_ids'],\n",
        "    preprocessed_data['attention_masks'],\n",
        "    preprocessed_data['labels']\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "model = setup_model()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"\\nData Split Information:\")\n",
        "print(f\"Training samples: {len(train_dataloader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataloader.dataset)}\")\n",
        "print(f\"\\nModel will run on: {device}\")\n",
        "print(f\"Number of batches in training: {len(train_dataloader)}\")\n",
        "print(f\"Number of batches in validation: {len(val_dataloader)}\")\n",
        "\n",
        "sample_batch = next(iter(train_dataloader))\n",
        "print(\"\\nBatch shape information:\")\n",
        "print(f\"Input IDs shape: {sample_batch[0].shape}\")\n",
        "print(f\"Attention mask shape: {sample_batch[1].shape}\")\n",
        "print(f\"Labels shape: {sample_batch[2].shape}\")\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "model, training_stats, best_accuracy = train_model(model, train_dataloader, val_dataloader)\n",
        "\n",
        "print(\"\\nFinal Training Stats:\")\n",
        "for stat in training_stats:\n",
        "    print(f\"Epoch {stat['epoch']}:\")\n",
        "    print(f\"  Training Loss: {stat['Training Loss']:.3f}\")\n",
        "    print(f\"  Validation Loss: {stat['Valid. Loss']:.3f}\")\n",
        "    print(f\"  Validation Accuracy: {stat['Valid. Accur.']:.3f}\")\n",
        "\n",
        "print(f\"\\nBest validation accuracy: {best_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"\\nStarting training...\")\n",
        "model, training_stats, best_accuracy = train_model(model, val_dataloader, val_dataloader)\n",
        "\n",
        "# Print final statistics\n",
        "print(\"\\nFinal Training Stats:\")\n",
        "for stat in training_stats:\n",
        "    print(f\"Epoch {stat['epoch']}:\")\n",
        "    print(f\"  Training Loss: {stat['Training Loss']:.3f}\")\n",
        "    print(f\"  Validation Loss: {stat['Valid. Loss']:.3f}\")\n",
        "    print(f\"  Validation Accuracy: {stat['Valid. Accur.']:.3f}\")\n",
        "\n",
        "print(f\"\\nBest validation accuracy: {best_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "zaq4RLWNmMKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_test_df = pd.read_csv(f\"{folder_path}/medical_tc_test.csv\")\n",
        "real_test_preprocessed = preprocess_for_biobert(real_test_df)\n",
        "real_test_dataloader = DataLoader(\n",
        "    TensorDataset(real_test_preprocessed['input_ids'], real_test_preprocessed['attention_masks'], real_test_preprocessed['labels']),\n",
        "    batch_size=16,\n",
        "    shuffle=False\n",
        ")\n",
        "model.eval()\n",
        "total_test_accuracy = 0\n",
        "for batch in real_test_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "    accuracy = (predictions == b_labels).float().mean()\n",
        "    total_test_accuracy += accuracy.item()\n",
        "    avg_test_accuracy = total_test_accuracy / len(real_test_dataloader)\n",
        "print(f\"Average Test Accuracy: {avg_test_accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwU_Y0yUpMRG",
        "outputId": "e66ffe54-fac5-4002-baec-5c9183fc1fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning medical abstracts...\n",
            "Encoding texts with BioBERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2888/2888 [00:03<00:00, 897.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Test Accuracy: 0.546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('../dataset/best_biobert_medical_classifier')"
      ],
      "metadata": {
        "id": "6h4ssc_Xq0SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Load the BioBERT model and move it to GPU\n",
        "biobert_model = AutoModel.from_pretrained('dmis-lab/biobert-v1.1').to('cuda')\n",
        "print(\"BioBERT model loaded on GPU!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u00U3k9dkN49",
        "outputId": "67f11848-6487-44ef-acd5-c3d0b4bfc118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BioBERT model loaded on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBkeH4fa1gkt",
        "outputId": "1f201d35-47d0-408b-c815-89866050ccee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing: 100%|██████████| 26550/26550 [00:31<00:00, 851.44it/s]\n",
            "Extracting embeddings: 100%|██████████| 830/830 [06:24<00:00,  2.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved to /content/drive/My Drive/embeddings.npy\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Load BioBERT model and tokenizer\n",
        "model_name = 'dmis-lab/biobert-v1.1\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Main execution\n",
        "folder_path = \"../dataset\"\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(f\"{folder_path}/combined_medical_dataset.csv\")\n",
        "\n",
        "texts = df['medical_abstract'].tolist()\n",
        "\n",
        "# Preprocess and tokenize the dataset\n",
        "def preprocess_and_tokenize(texts, tokenizer, max_length=512):\n",
        "    \"\"\"Tokenize all texts and return input IDs and attention masks.\"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# Tokenize the full dataset\n",
        "input_ids, attention_masks = preprocess_and_tokenize(texts, tokenizer)\n",
        "\n",
        "def extract_embeddings(input_ids, attention_masks, model, batch_size=32):\n",
        "    \"\"\"Extract embeddings in batches to handle large datasets.\"\"\"\n",
        "    embeddings = []\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(input_ids), batch_size), desc=\"Extracting embeddings\"):\n",
        "            batch_input_ids = input_ids[i:i + batch_size].to('cuda')\n",
        "            batch_attention_masks = attention_masks[i:i + batch_size].to('cuda')\n",
        "            outputs = model(batch_input_ids, attention_mask=batch_attention_masks, output_hidden_states=True)\n",
        "            # Now you can access hidden states:\n",
        "            cls_embeddings = outputs.hidden_states[-1][:, 0, :]\n",
        "            embeddings.append(cls_embeddings.cpu())\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "embeddings = extract_embeddings(input_ids, attention_masks, model)\n",
        "\n",
        "embeddings_np = embeddings.numpy()\n",
        "\n",
        "output_path = '../embeddings.npy'\n",
        "np.save(output_path, embeddings_np)\n",
        "print(f\"Embeddings saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(f\"{folder_path}/medical_tc_test.csv\")\n",
        "texts = test_df['medical_abstract'].tolist()\n",
        "test_input_ids, test_attention_masks = preprocess_and_tokenize(texts, tokenizer)\n",
        "test_embeddings = extract_embeddings(test_input_ids, test_attention_masks, model)\n",
        "test_embeddings_np = test_embeddings.numpy()\n",
        "np.save('../test_embeddings.npy', test_embeddings_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_leFCHbks-rW",
        "outputId": "58a7ce89-9e07-499a-fd25-856744937ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing: 100%|██████████| 2888/2888 [00:03<00:00, 840.77it/s]\n",
            "Extracting embeddings: 100%|██████████| 91/91 [00:42<00:00,  2.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "embeddings_path = '../embeddings.npy'\n",
        "embeddings = np.load(embeddings_path)\n",
        "\n",
        "\n",
        "train_labels = df['condition_label'].values - 1\n",
        "\n",
        "test_embeddings_path = '../test_embeddings.npy'\n",
        "test_embeddings = np.load(test_embeddings_path)\n",
        "test_labels = test_df['condition_label'].values - 1\n",
        "\n",
        "\n",
        "X_train = embeddings\n",
        "y_train = train_labels\n",
        "X_test = test_embeddings\n",
        "y_test = test_labels\n",
        "\n",
        "# Logistic Regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "lr_preds = lr.predict(X_test)\n",
        "print(\"\\nLogistic Regression Results:\")\n",
        "print(classification_report(y_test, lr_preds))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, lr_preds))\n",
        "\n",
        "# Random Forest\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_preds = rf.predict(X_test)\n",
        "print(\"\\nRandom Forest Results:\")\n",
        "print(classification_report(y_test, rf_preds))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, rf_preds))\n",
        "\n",
        "# Support Vector Machine\n",
        "print(\"\\nTraining Support Vector Machine...\")\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "svm_preds = svm.predict(X_test)\n",
        "print(\"\\nSVM Results:\")\n",
        "print(classification_report(y_test, svm_preds))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, svm_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO7Jn1zXo5dI",
        "outputId": "126fb3a7-c5e5-4c62-ece0-c58cfda9558f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression...\n",
            "\n",
            "Logistic Regression Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.65      0.66       633\n",
            "           1       0.42      0.46      0.44       299\n",
            "           2       0.49      0.47      0.48       385\n",
            "           3       0.60      0.61      0.61       610\n",
            "           4       0.42      0.42      0.42       961\n",
            "\n",
            "    accuracy                           0.52      2888\n",
            "   macro avg       0.52      0.52      0.52      2888\n",
            "weighted avg       0.52      0.52      0.52      2888\n",
            "\n",
            "Accuracy: 0.521814404432133\n",
            "\n",
            "Training Random Forest...\n",
            "\n",
            "Random Forest Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.66      0.66       633\n",
            "           1       0.43      0.45      0.44       299\n",
            "           2       0.50      0.45      0.48       385\n",
            "           3       0.60      0.61      0.61       610\n",
            "           4       0.42      0.42      0.42       961\n",
            "\n",
            "    accuracy                           0.52      2888\n",
            "   macro avg       0.52      0.52      0.52      2888\n",
            "weighted avg       0.52      0.52      0.52      2888\n",
            "\n",
            "Accuracy: 0.5225069252077562\n",
            "\n",
            "Training Support Vector Machine...\n",
            "\n",
            "SVM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.64      0.64       633\n",
            "           1       0.42      0.46      0.44       299\n",
            "           2       0.49      0.47      0.48       385\n",
            "           3       0.60      0.61      0.61       610\n",
            "           4       0.41      0.40      0.40       961\n",
            "\n",
            "    accuracy                           0.51      2888\n",
            "   macro avg       0.51      0.51      0.51      2888\n",
            "weighted avg       0.51      0.51      0.51      2888\n",
            "\n",
            "Accuracy: 0.5107340720221607\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}